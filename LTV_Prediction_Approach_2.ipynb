{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b0bf1b-6f5e-4e73-aac5-895da11614d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Attempts at improving model predictions:\n",
    "\n",
    "1. Could use bootstrapping due to dearth of data\n",
    "\n",
    "2. Could use Ensemble methods(XGBoostRegressor, RandomForestRegressor)\n",
    "\n",
    "3. Could use Blended Models/Stacking: Combine your best models. (Ridge + XGBoost + MLP) â†’ feed their outputs into a final LinearRegression()\n",
    "\t\tfrom sklearn.ensemble import StackingRegressor\n",
    "\t\testimators = [('ridge', ridge), ('mlp', mlp), ('xgb', xgb)]\n",
    "\t\tstack = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\n",
    "\n",
    "4. Adding in more variables(predictors):\n",
    "\t\tCustomer age (in days) = Date of last purchase - first purchase\n",
    "\t\tTime since last purchase\n",
    "\t\tPurchase seasonality = Month of cohort or mode of purchase month\n",
    "\t\tAvg order value, purchase frequency per month\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e79846d-34fc-4fcc-9acd-bad9900e9061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7422b48-8bec-4f50-8f6a-56896c1bc0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from scipy import stats\n",
    "from sklearn.base import clone\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d0eff-1386-482d-a60f-4c5f43b8dad9",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec4c41b-0405-4efc-aaee-35698e1b59a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.12)\n",
      "Path to dataset files: /Users/rohanojha/.cache/kagglehub/datasets/mashlyn/online-retail-ii-uci/versions/3\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/datasets/mashlyn/online-retail-ii-uci\n",
    "path = kagglehub.dataset_download(\"mashlyn/online-retail-ii-uci\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "file_path = os.path.join(path, 'online_retail_II.csv')\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Loading and preprocessing the data\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "# Dropping rows with CustomerID as NULL\n",
    "df = df.dropna(subset=['Customer ID'])\n",
    "\n",
    "# Capping Outliers\n",
    "df = df[(df['Quantity'] > 0) & (df['Quantity'] < df['Quantity'].quantile(0.99))]\n",
    "\n",
    "# Defining Training cohorts: Considering Dec-2010, Jan-2011, Feb-2011 and Mar-2011 as training sample cohorts\n",
    "training_cohorts = [\n",
    "    ('2010-12-01', '2009-12-01', '2010-11-30', '2011-01-01', '2011-06-30'),\n",
    "    ('2011-01-01', '2010-01-01', '2010-12-31', '2011-02-01', '2011-07-31'),\n",
    "    ('2011-02-01', '2010-02-01', '2011-01-31', '2011-03-01', '2011-08-31'),\n",
    "    ('2011-03-01', '2010-03-01', '2011-02-28', '2011-04-01', '2011-09-30')]\n",
    "\n",
    "# Defining Out-Of-Time cohorts. Dataset ends ~Dec 9, 2011\n",
    "holdout_cohorts = [\n",
    "    ('2011-04-01', '2010-04-01', '2011-03-31', '2011-05-01', '2011-10-31'),\n",
    "    ('2011-05-01', '2010-05-01', '2011-04-30', '2011-06-01', '2011-11-30'),\n",
    "    ('2011-06-01', '2010-06-01', '2011-05-31', '2011-07-01', '2011-12-09')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409677e2-622c-42a7-94ba-1612c0f90963",
   "metadata": {},
   "source": [
    "### Function to calculate the RFM variables and LTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5c9f556-c229-4ed9-8851-d60cd24db16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rfm_ltv(cohort_month, train_start, train_end, test_start, test_end, is_holdout=False, training_scaler=None):\n",
    "    cohort_month = pd.to_datetime(cohort_month)\n",
    "    train_start = pd.to_datetime(train_start)\n",
    "    train_end = pd.to_datetime(train_end)\n",
    "    test_start = pd.to_datetime(test_start)\n",
    "    test_end = pd.to_datetime(test_end)\n",
    "    \n",
    "    cohort_customers = df[(df['InvoiceDate'] >= cohort_month) & \n",
    "                         (df['InvoiceDate'] < cohort_month + pd.offsets.MonthEnd(1))]['Customer ID'].unique()\n",
    "    \n",
    "    ###################### Filtering customers with sufficient lookback data ######################\n",
    "    train = df[(df['InvoiceDate'] >= train_start) & (df['InvoiceDate'] <= train_end) & \n",
    "               (df['Customer ID'].isin(cohort_customers))]\n",
    "    customer_transaction_counts = train.groupby('Customer ID')['Invoice'].nunique()\n",
    "    valid_customers = customer_transaction_counts[customer_transaction_counts >= 2].index\n",
    "    train = train[train['Customer ID'].isin(valid_customers)]\n",
    "    cohort_customers = set(cohort_customers).intersection(valid_customers)\n",
    "\n",
    "    ###################### Obtaining the country coalesced variable. This can be used in PCA to form clusters ######################\n",
    "    country_map = {\n",
    "        'United Kingdom': 'UK', 'EIRE': 'Europe', 'Germany': 'Europe', 'France': 'Europe', 'Belgium': 'Europe',\n",
    "        'Portugal': 'Europe', 'Netherlands': 'Europe', 'Poland': 'Europe', 'Channel Islands': 'Europe',\n",
    "        'Spain': 'Europe', 'Cyprus': 'Europe', 'Greece': 'Europe', 'Norway': 'Europe', 'Austria': 'Europe',\n",
    "        'Sweden': 'Europe', 'Finland': 'Europe', 'Denmark': 'Europe', 'Italy': 'Europe', 'Switzerland': 'Europe',\n",
    "        'Lithuania': 'Europe', 'Czech Republic': 'Europe', 'USA': 'North America', 'Canada': 'North America',\n",
    "        'Australia': 'Asia-Pacific', 'Japan': 'Asia-Pacific', 'Singapore': 'Asia-Pacific', 'Thailand': 'Asia-Pacific',\n",
    "        'Korea': 'Asia-Pacific', 'United Arab Emirates': 'Middle East & Africa', 'Nigeria': 'Middle East & Africa',\n",
    "        'RSA': 'Middle East & Africa', 'Bahrain': 'Middle East & Africa', 'Israel': 'Middle East & Africa',\n",
    "        'Saudi Arabia': 'Middle East & Africa', 'Lebanon': 'Middle East & Africa', 'Malta': 'Europe',\n",
    "        'Iceland': 'Europe', 'Brazil': 'Others', 'West Indies': 'Others', 'Unspecified': 'Others',\n",
    "        'European Community': 'Others'}\n",
    "    train['Country_coalesced'] = train['Country'].map(country_map).fillna('Others')\n",
    "    train['Country_encoded'] = train['Country_coalesced'].map(train['Country_coalesced'].value_counts(normalize=True))\n",
    "\n",
    "    ###################### Computing the RFM predictors ######################\n",
    "    rfm = train.groupby('Customer ID').agg({'InvoiceDate': [lambda x: (train_end - x.max()).days,\n",
    "                                                            lambda x: (x.max() - x.min()).days,\n",
    "                                                            lambda x: stats.mode(x.dt.month)[0]],\n",
    "        'Invoice': 'nunique', 'Price': lambda x: (x * train.loc[x.index, 'Quantity']).sum(), 'Quantity': 'sum'})\n",
    "\n",
    "    ###################### Flattening column names ######################\n",
    "    rfm.columns = ['Recency', 'CustomerAge', 'Seasonality', 'Frequency', 'Monetary', 'TotalQuantity']\n",
    "    \n",
    "    ###################### Calculating Average Order Value ######################\n",
    "    rfm['AvgOrderValue'] = rfm['Monetary'] / rfm['Frequency']\n",
    "    \n",
    "    ###################### Calculating Purchase Frequency per Month ######################\n",
    "    training_months = (train_end - train_start).days / 30\n",
    "    rfm['FreqPerMonth'] = rfm['Frequency'] / training_months\n",
    "    \n",
    "    ###################### Adding Country_encoded ######################\n",
    "    rfm['Country_encoded'] = train.groupby('Customer ID')['Country_encoded'].first()\n",
    "    \n",
    "    ###################### Log-transforming Monetary, AvgOrderValue to reduce skewness ######################\n",
    "    rfm['Monetary'] = np.log1p(rfm['Monetary'])\n",
    "    rfm['AvgOrderValue'] = np.log1p(rfm['AvgOrderValue'])\n",
    "    \n",
    "    ###################### Computing LTV (predictor) ######################\n",
    "    test = df[(df['InvoiceDate'] >= test_start) & (df['InvoiceDate'] <= test_end) & \n",
    "              (df['Customer ID'].isin(cohort_customers))]\n",
    "    ltv = test.groupby('Customer ID')[['Quantity', 'Price']].apply(\n",
    "        lambda x: (x['Quantity'] * x['Price']).sum(), include_groups=False).rename('LTV')\n",
    "    rfm = rfm.join(ltv, how='left').fillna(0)\n",
    "    rfm['LTV'] = np.log1p(rfm['LTV'])\n",
    "    \n",
    "    ###################### Applying Scaler Transformation if holdout sample ######################\n",
    "    features = ['Recency', 'CustomerAge', 'Seasonality', 'Frequency', 'Monetary', 'TotalQuantity', 'AvgOrderValue', 'FreqPerMonth', 'Country_encoded']\n",
    "    if is_holdout:\n",
    "        rfm_scaled = training_scaler.transform(rfm[features])\n",
    "        rfm[features] = rfm_scaled\n",
    "        return rfm\n",
    "    else:\n",
    "        return rfm, features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefeeae6-e944-48d7-aa47-4d8384dc643f",
   "metadata": {},
   "source": [
    "### Processing Training Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d20b5388-ce0d-460f-babf-7b8cfa14e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "ltv_scaler = StandardScaler()\n",
    "df_rfm = pd.DataFrame()\n",
    "\n",
    "for cohort_month, train_start, train_end, test_start, test_end in training_cohorts:\n",
    "    rfm, features = calc_rfm_ltv(cohort_month, train_start, train_end, test_start, test_end)\n",
    "    df_rfm = pd.concat([df_rfm, rfm.assign(Cohort=cohort_month)], ignore_index=True)\n",
    "\n",
    "###################### Standardizing predictors and target ######################\n",
    "df_rfm_scaled = scaler.fit_transform(df_rfm[features])\n",
    "df_rfm[features] = df_rfm_scaled\n",
    "df_rfm['LTV'] = ltv_scaler.fit_transform(df_rfm[['LTV']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86888796-813a-4c22-901e-319cd545ef60",
   "metadata": {},
   "source": [
    "### Bootstrap Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c1b6bb47-c825-448f-82c3-50280fb0205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(df, n_samples):\n",
    "    sample_size = len(df)\n",
    "    bootstrap_dfs = []\n",
    "    for _ in range(n_samples):\n",
    "        sample = df.sample(n=sample_size, replace=True)\n",
    "        bootstrap_dfs.append(sample)\n",
    "    return bootstrap_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36db7c6-5c92-431a-bd07-7d9787b310cf",
   "metadata": {},
   "source": [
    "### Training Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc7b9418-584d-4f87-acf7-7da940fa1e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Performance:\n",
      "\n",
      "RandomForest RMSE: 0.5528353171185957\n",
      "RandomForest MAE: 0.38524672649147207\n",
      "RandomForest R2: 0.6943731121463816\n",
      "XGBoost RMSE: 0.7332312018246462\n",
      "XGBoost MAE: 0.5121995157236068\n",
      "XGBoost R2: 0.462372004670785\n",
      "Stacked Ensemble RMSE: 0.639289703041117\n",
      "Stacked Ensemble MAE: 0.4466068282192547\n",
      "Stacked Ensemble R2: 0.5913086755856004\n"
     ]
    }
   ],
   "source": [
    "def train_ensemble_models(df_rfm, features, n_bootstrap=100):\n",
    "    X = df_rfm[features]\n",
    "    y = df_rfm['LTV']\n",
    "\n",
    "    rf_model = RandomForestRegressor(random_state=2)\n",
    "    xgb_model = XGBRegressor(random_state=2)\n",
    "\n",
    "    ###################### Hyperparameters ######################\n",
    "    rf_param_grid = {'n_estimators': [100, 200], 'max_depth': [10, 20, None], 'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]}\n",
    "    xgb_param_grid = {'n_estimators': [100, 200], 'max_depth': [3, 6], 'learning_rate': [0.01, 0.1], 'reg_lambda': [1, 10]}\n",
    "\n",
    "    ###################### Performing GridSearchCV to find the required hypermeters ######################\n",
    "    rf_grid  = GridSearchCV(rf_model,  rf_param_grid,  cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    xgb_grid = GridSearchCV(xgb_model, xgb_param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "    ###################### Fitting the models ######################\n",
    "    rf_grid.fit(X, y)\n",
    "    xgb_grid.fit(X, y)\n",
    "\n",
    "    ###################### Choosing the Best Models ######################\n",
    "    rf_model = rf_grid.best_estimator_\n",
    "    xgb_model = xgb_grid.best_estimator_\n",
    "    \n",
    "    ###################### Bootstrapping training sample ######################\n",
    "    # rf_predictions = []\n",
    "    # xgb_predictions = []\n",
    "    # bootstrap_samples = bootstrap_sample(df_rfm, n_samples=n_bootstrap)\n",
    "    \n",
    "    # for sample in bootstrap_samples:\n",
    "    #     X_sample = sample[features]\n",
    "    #     y_sample = sample['LTV']\n",
    "        \n",
    "        ###################### Training RandomForest ######################\n",
    "        # rf_model.fit(X_sample, y_sample)\n",
    "        # rf_pred = rf_model.predict(X)\n",
    "        # rf_predictions.append(rf_pred)\n",
    "        \n",
    "        ###################### Training XGBoost ######################\n",
    "        # xgb_model.fit(X_sample, y_sample)\n",
    "        # xgb_pred = xgb_model.predict(X)\n",
    "        # xgb_predictions.append(xgb_pred)\n",
    "    \n",
    "    ###################### Averaging predictions across the bootstrap samples ######################\n",
    "    # rf_pred_avg = np.mean(rf_predictions, axis=0)\n",
    "    # xgb_pred_avg = np.mean(xgb_predictions, axis=0)\n",
    "    # final_pred = (rf_pred_avg + xgb_pred_avg) / 2\n",
    "    \n",
    "    ###################### Predictions ######################\n",
    "    rf_pred = rf_model.predict(X)\n",
    "    xgb_pred = xgb_model.predict(X)\n",
    "    final_pred = (rf_pred + xgb_pred) / 2\n",
    "\n",
    "    ###################### Training performance ######################\n",
    "    print(\"\\nTraining Set Performance:\\n\")\n",
    "    print(\"RandomForest RMSE:\", mean_squared_error(y, rf_pred, squared=False))\n",
    "    print(\"RandomForest MAE:\", mean_absolute_error(y, rf_pred))\n",
    "    print(\"RandomForest R2:\", r2_score(y, rf_pred))\n",
    "    \n",
    "    print(\"XGBoost RMSE:\", mean_squared_error(y, xgb_pred, squared=False))\n",
    "    print(\"XGBoost MAE:\", mean_absolute_error(y, xgb_pred))\n",
    "    print(\"XGBoost R2:\", r2_score(y, xgb_pred))\n",
    "    \n",
    "    print(\"Stacked Ensemble RMSE:\", mean_squared_error(y, final_pred, squared=False))\n",
    "    print(\"Stacked Ensemble MAE:\", mean_absolute_error(y, final_pred))\n",
    "    print(\"Stacked Ensemble R2:\", r2_score(y, final_pred))\n",
    "    \n",
    "    return rf_model, xgb_model, final_pred\n",
    "\n",
    "###################### Invoking train_ensemble_model() ######################\n",
    "rf_model, xgb_model, final_pred = train_ensemble_models(df_rfm, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "016b0ca0-5865-48ca-a119-8175a880b9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>XGB_Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monetary</td>\n",
       "      <td>0.561921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Frequency</td>\n",
       "      <td>0.141378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TotalQuantity</td>\n",
       "      <td>0.074810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Country_encoded</td>\n",
       "      <td>0.055909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recency</td>\n",
       "      <td>0.051270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AvgOrderValue</td>\n",
       "      <td>0.040671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CustomerAge</td>\n",
       "      <td>0.038242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seasonality</td>\n",
       "      <td>0.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FreqPerMonth</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Feature  XGB_Importance\n",
       "4         Monetary        0.561921\n",
       "3        Frequency        0.141378\n",
       "5    TotalQuantity        0.074810\n",
       "8  Country_encoded        0.055909\n",
       "0          Recency        0.051270\n",
       "6    AvgOrderValue        0.040671\n",
       "1      CustomerAge        0.038242\n",
       "2      Seasonality        0.035800\n",
       "7     FreqPerMonth        0.000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_importances = xgb_model.feature_importances_\n",
    "importance_df = pd.DataFrame({'Feature': features, 'XGB_Importance': xgb_importances}).sort_values(by='XGB_Importance', ascending=False)\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bfb7ad-1345-4132-967a-0b390407fd89",
   "metadata": {},
   "source": [
    "### Processing Holdout Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "716c341a-b3d8-469c-a027-c29aca8884ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Holdout Set Performance:\n",
      "\n",
      "RandomForest RMSE: 0.7668983148741471\n",
      "RandomForest MAE: 0.5765329488601766\n",
      "RandomForest R2: 0.26841321268188423\n",
      "XGBoost RMSE: 0.7330994880677224\n",
      "XGBoost MAE: 0.5436879629820903\n",
      "XGBoost R2: 0.3314773610163301\n",
      "Stacked Ensemble RMSE: 0.7439178444637154\n",
      "Stacked Ensemble MAE: 0.5572196383729554\n",
      "Stacked Ensemble R2: 0.3116009868911104\n"
     ]
    }
   ],
   "source": [
    "df_holdout = pd.DataFrame()\n",
    "for cohort_month, train_start, train_end, test_start, test_end in holdout_cohorts:\n",
    "    rfm = calc_rfm_ltv(cohort_month, train_start, train_end, test_start, test_end, is_holdout=True, training_scaler=scaler)\n",
    "    df_holdout = pd.concat([df_holdout, rfm.assign(Cohort=cohort_month)], ignore_index=True)\n",
    "\n",
    "# Standardizing LTV for holdout\n",
    "df_holdout['LTV'] = ltv_scaler.transform(df_holdout[['LTV']])\n",
    "\n",
    "# Predicting on the holdout data\n",
    "X_holdout = df_holdout[features]\n",
    "rf_pred_holdout = rf_model.predict(X_holdout)\n",
    "xgb_pred_holdout = xgb_model.predict(X_holdout)\n",
    "final_pred_holdout = (rf_pred_holdout + xgb_pred_holdout) / 2\n",
    "\n",
    "# Evaluating the holdout performance\n",
    "print(\"\\nHoldout Set Performance:\\n\")\n",
    "print(\"RandomForest RMSE:\", mean_squared_error(df_holdout['LTV'], rf_pred_holdout, squared=False))\n",
    "print(\"RandomForest MAE:\", mean_absolute_error(df_holdout['LTV'], rf_pred_holdout))\n",
    "print(\"RandomForest R2:\", r2_score(df_holdout['LTV'], rf_pred_holdout))\n",
    "print(\"XGBoost RMSE:\", mean_squared_error(df_holdout['LTV'], xgb_pred_holdout, squared=False))\n",
    "print(\"XGBoost MAE:\", mean_absolute_error(df_holdout['LTV'], xgb_pred_holdout))\n",
    "print(\"XGBoost R2:\", r2_score(df_holdout['LTV'], xgb_pred_holdout))\n",
    "print(\"Stacked Ensemble RMSE:\", mean_squared_error(df_holdout['LTV'], final_pred_holdout, squared=False))\n",
    "print(\"Stacked Ensemble MAE:\", mean_absolute_error(df_holdout['LTV'], final_pred_holdout))\n",
    "print(\"Stacked Ensemble R2:\", r2_score(df_holdout['LTV'], final_pred_holdout))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
